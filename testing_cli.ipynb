{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoConfig, AddedToken, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict\n",
    "import torch\n",
    "import copy\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 聊天模板"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Template:\n",
    "    template_name:str\n",
    "    system_format: str\n",
    "    user_format: str\n",
    "    assistant_format: str\n",
    "    system: str\n",
    "    stop_word: str\n",
    "\n",
    "template_dict: Dict[str, Template] = dict()\n",
    "\n",
    "def register_template(template_name, system_format, user_format, assistant_format, system, stop_word=None):\n",
    "    template_dict[template_name] = Template(\n",
    "        template_name=template_name,\n",
    "        system_format=system_format,\n",
    "        user_format=user_format,\n",
    "        assistant_format=assistant_format,\n",
    "        system=system,\n",
    "        stop_word=stop_word,\n",
    "    )\n",
    "\n",
    "# 这里的系统提示词是训练时使用的，推理时可以自行尝试修改效果\n",
    "register_template(\n",
    "    template_name='llama3',\n",
    "    system_format='<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{content}<|eot_id|>',\n",
    "    user_format='<|start_header_id|>user<|end_header_id|>\\n\\n{content}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n',\n",
    "    assistant_format='{content}<|eot_id|>',\n",
    "    # template_name='llama3',\n",
    "    # system_format='<|begin_of_text|><<SYS>>\\n{content}\\n<</SYS>>\\n\\n<|eot_id|>',\n",
    "    # user_format='<|start_header_id|>user<|end_header_id|>\\n\\n{content}<|eot_id|>',\n",
    "    # assistant_format='<|start_header_id|>assistant<|end_header_id|>\\n\\n{content}\\n', # \\n\\n{content}<|eot_id|>\\n\n",
    "    system=\"你是一個心理健康助手EmoModel，你旨在通過專業心理諮詢，協助來訪者完成心理診斷。請充分利用專業心理學知識與諮詢技術，一步步幫助來訪者解決心理問題。\",\n",
    "    stop_word='<|eot_id|>'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 加载模型\n",
    "def load_model(model_name_or_path, load_in_4bit=False, adapter_name_or_path=None):\n",
    "    if load_in_4bit:\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            llm_int8_threshold=6.0,\n",
    "            llm_int8_has_fp16_weight=False,\n",
    "        )\n",
    "    else:\n",
    "        quantization_config = None\n",
    "\n",
    "    # 加载base model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        load_in_4bit=load_in_4bit,\n",
    "        trust_remote_code=True,\n",
    "        low_cpu_mem_usage=True,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map='auto',\n",
    "        quantization_config=quantization_config\n",
    "    )\n",
    "\n",
    "    # 加载adapter\n",
    "    if adapter_name_or_path is not None:\n",
    "        model = PeftModel.from_pretrained(model, adapter_name_or_path)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokenizer(model_name_or_path):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        trust_remote_code=True,\n",
    "        use_fast=False\n",
    "    )\n",
    "\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## promprt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 构建prompt\n",
    "def build_prompt(tokenizer, template, query, history, system=None):\n",
    "    template_name = template.template_name\n",
    "    system_format = template.system_format\n",
    "    user_format = template.user_format\n",
    "    assistant_format = template.assistant_format\n",
    "    system = system if system is not None else template.system\n",
    "\n",
    "    history.append({\"role\": 'user', 'message': query})\n",
    "    input_ids = []\n",
    "\n",
    "    # 添加系统信息\n",
    "    if system_format is not None:\n",
    "        if system is not None:\n",
    "            system_text = system_format.format(content=system)\n",
    "            input_ids = tokenizer.encode(system_text, add_special_tokens=False)\n",
    "    # 拼接历史对话\n",
    "    for item in history:\n",
    "        role, message = item['role'], item['message']\n",
    "        if role == 'user':\n",
    "            message = user_format.format(content=message, stop_token=tokenizer.eos_token)\n",
    "        else:\n",
    "            message = assistant_format.format(content=message, stop_token=tokenizer.eos_token)\n",
    "        tokens = tokenizer.encode(message, add_special_tokens=False)\n",
    "        input_ids += tokens\n",
    "    input_ids = torch.tensor([input_ids], dtype=torch.long)\n",
    "\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 主程式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    # download model in openxlab\n",
    "    # download(model_repo='MrCat/Meta-Llama-3-8B-Instruct', \n",
    "    #        output='MrCat/Meta-Llama-3-8B-Instruct')\n",
    "    # model_name_or_path = 'MrCat/Meta-Llama-3-8B-Instruct'\n",
    "\n",
    "    # # download model in modelscope\n",
    "    # model_name_or_path = snapshot_download('LLM-Research/Meta-Llama-3-8B-Instruct', \n",
    "    #                                        cache_dir='LLM-Research/Meta-Llama-3-8B-Instruct')\n",
    "\n",
    "    # offline model\n",
    "    model_name_or_path = r\"./merged_Llama3_8b\"\n",
    "\n",
    "    print_user = True # 控制是否输入提示输入框，用于notebook时，改为True\n",
    "\n",
    "    template_name = 'llama3'\n",
    "    adapter_name_or_path = None\n",
    "\n",
    "    template = template_dict[template_name]    \n",
    "\n",
    "    # 若开启4bit推理能够节省很多显存，但效果可能下降\n",
    "    load_in_4bit = False\n",
    "\n",
    "    # 生成超参配置，可修改以取得更好的效果\n",
    "    max_new_tokens = 500 # 每次回复时，AI生成文本的最大长度\n",
    "    top_p = 0.9\n",
    "    temperature = 0.6 # 越大越有创造性，越小越保守\n",
    "    repetition_penalty = 1.1 # 越大越能避免吐字重复\n",
    "\n",
    "    # 加载模型\n",
    "    print(f'Loading model from: {model_name_or_path}')\n",
    "    print(f'adapter_name_or_path: {adapter_name_or_path}')\n",
    "    model = load_model(\n",
    "        model_name_or_path,\n",
    "        load_in_4bit=load_in_4bit,\n",
    "        adapter_name_or_path=adapter_name_or_path\n",
    "    ).eval()\n",
    "    tokenizer = load_tokenizer(model_name_or_path if adapter_name_or_path is None else adapter_name_or_path)\n",
    "    # print(tokenizer)\n",
    "    if template.stop_word is None:\n",
    "        template.stop_word = tokenizer.eos_token\n",
    "    stop_token_id = tokenizer.encode(template.stop_word, add_special_tokens=True)\n",
    "    print(stop_token_id)\n",
    "    assert len(stop_token_id) == 2\n",
    "    stop_token_id = stop_token_id[1]\n",
    "\n",
    "\n",
    "    print(\"================================================================================\")\n",
    "    print(\"=============欢迎来到Llama3 EmoLLM 心理咨询室, 输入'exit'退出程序==============\")\n",
    "    print(\"================================================================================\")\n",
    "    history = []\n",
    "\n",
    "    print('=======================请输入咨询或聊天内容, 按回车键结束=======================')\n",
    "    print(\"================================================================================\")\n",
    "    print(\"================================================================================\")\n",
    "    print(\"===============================让我们开启对话吧=================================\\n\\n\")\n",
    "    if print_user:\n",
    "        query = input('用户:')\n",
    "        print(\"# 用户：{}\".format(query))\n",
    "    else:\n",
    "        \n",
    "        query = input('# 用户: ')\n",
    "        \n",
    "    while True:\n",
    "        if query=='exit':\n",
    "            break\n",
    "        query = query.strip()\n",
    "        input_ids = build_prompt(tokenizer, template, query, copy.deepcopy(history), system=None).to(model.device)\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids, max_new_tokens=max_new_tokens, do_sample=True,\n",
    "            top_p=top_p, temperature=temperature, repetition_penalty=repetition_penalty,\n",
    "            eos_token_id=stop_token_id, pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        outputs = outputs.tolist()[0][len(input_ids[0]):]\n",
    "        response = tokenizer.decode(outputs)\n",
    "        response = response.strip().replace(template.stop_word, \"\").strip()\n",
    "\n",
    "        # 存储对话历史\n",
    "        history.append({\"role\": 'user', 'message': query})\n",
    "        history.append({\"role\": 'assistant', 'message': response})\n",
    "\n",
    "        # 当对话长度超过6轮时，清空最早的对话，可自行修改\n",
    "        if len(history) > 12:\n",
    "            history = history[:-12]\n",
    "\n",
    "        print(\"# Llama3 EmoLLM 心理咨询师：{}\".format(response.replace('\\n','').replace('<|start_header_id|>','').replace('assistant<|end_header_id|>','')))\n",
    "        print()\n",
    "        query = input('# 用户：')\n",
    "        if print_user:\n",
    "            print(\"# 用户：{}\".format(query))\n",
    "    print(\"\\n\\n=============感谢使用Llama3 EmoLLM 心理咨询室, 祝您生活愉快~=============\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 測試"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: ./merged_Llama3_8b\n",
      "adapter_name_or_path: None\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "856303222558488c9156c7df0a8a7a20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[128000, 128009]\n",
      "================================================================================\n",
      "=============欢迎来到Llama3 EmoLLM 心理咨询室, 输入'exit'退出程序==============\n",
      "================================================================================\n",
      "=======================请输入咨询或聊天内容, 按回车键结束=======================\n",
      "================================================================================\n",
      "================================================================================\n",
      "===============================让我们开启对话吧=================================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
